# Compile:
squeue -u tarch
scancel -u tarch --state=running

cd /fslhome/tarch/compute/673/word-rnn-tensorflow

salloc --mem 32000M --time 1:00:00 --gres=gpu:0
module purge
module load defaultenv
module load cuda/8.0
module load cudnn/5.1_cuda-8.0
module load python/2/7
cd /fslhome/tarch/compute/673/word-rnn-tensorflow2

module load tensorflow/0.9.0_python-3.4.4+cuda

python ./sample.py -e princess -o sample.txt -s ./save/FINAL --pick 1 -n 200 --prime "kingdom" -y 10

python ./sample.py -e fife -o sample.txt -s ./save/FINAL --pick 1 -n 200 --prime "sun" -y 10

python ./sample.py -e $'\n' -o sample.txt -s ./save/FINAL --pick 1 -n 200 --prime "light" -y 10


module purge
module load python/2/7


module load python/3/6
module load anaconda/3/4.3.1



# For scikit etc.:
module load anaconda/3/4.3.1
conda create --name TAnaconda --clone anaconda/3/4.3.1
    

module purge
module load use.own 
module load anacondaTA
nano ~/.bashrc

ANACONDA:
export PATH=~/anaconda3/bin:$PATH
conda search "^python$"
conda create --name conda3tf2 python=3
source activate conda3tf2

# INSTALL
conda install -c anaconda tensorflow-gpu 
conda install -c anaconda gym
conda install -c anaconda matplotlib

OR 
module load anaconda/3/4.3.1
pip install --user gym


module show anaconda/3/4.3.1
module purge
module load fslhome/tarch/.conda/envs/my_root


pip install --user tensorflow
pip install --user package
pip install --user gensim -U

pip3 install --user package

salloc --mem 16000M --time 2:00:00 --gres=gpu:4



# Main training
python3 train.py --data_dir ./data --rnn_size 256 --num_layers 2 --model lstm --batch_size 50 --seq_length 50 --num_epochs 5000


python train.py --data_dir ./data/large --rnn_size 256 --num_layers 2 --model lstm --batch_size 512 --seq_length 50 --num_epochs 5000 --init_from "./save/NOBONUS"

python train.py --data_dir ./data/original --rnn_size 256 --num_layers 2 --model lstm --batch_size 256 --seq_length 30 --num_epochs 5000 --sample true --bonus true --save_dir "./save/THEBONUS"


python train.py --data_dir ./data/original --rnn_size 256 --num_layers 2 --model lstm --batch_size 256 --seq_length 30 --num_epochs 5000 --sample true --bonus true --save_dir "./save/THEBONUSX" --init_from "./save/THEBONUSX" --end_word_training True






python train.py --data_dir ./data/original --rnn_size 256 --num_layers 2 --model lstm --batch_size 256 --seq_length 30 --num_epochs 5000 --sample true --bonus false --save_dir "./save/NOBONUS" --init_from "./save/NOBONUS"


# THEBONUS
python sample.py -e turtle -o sample.txt -s ./save/THEBONUSX5 --pick 1

cd /fslhome/tarch/compute/673/word-rnn-tensorflow
python ./sample.py -e turtle -o sample.txt -s ./save/THEBONUS --pick 1 -n 200


python ./sample.py -e turtle -o sample.txt -s ./save/THEBONUS --pick 1 -n 200


--init_from "./save/NOBONUS"

sbatch ./train.sh

# NO BONUS
python sample.py -e turtle -o sample.txt -s ./save/NOBONUSX1 --pick 1


# Process
python3 ./processing/5_compile.py


# RESTORE
python train.py --data_dir ./data/original --rnn_size 256 --num_layers 2 --model lstm --batch_size 256 --seq_length 30 --num_epochs 5000 --sample true --bonus true --save_dir "./save/THEBONUS" --init_from "./save/THEBONUS"


# RESET
cd /fslhome/tarch/compute/673/wordrnn
git clone https://github.com/Tahlor/wordrnn
cp /fslhome/tarch/compute/673/word-rnn-tensorflow/data /fslhome/tarch/compute/673/wordrnn/data

## NO BONUS
python train.py --data_dir ./data/large --rnn_size 256 --num_layers 2 --model lstm --batch_size 256 --seq_length 30 --num_epochs 5000 --sample false --bonus false --save_dir "./save/NOBONUSX" 

sbatch ./train2.sh




## Original
salloc --mem 32000M --time 2:00:00 --gres=gpu:2

module purge
module load defaultenv
module load cuda/8.0
module load cudnn/5.1_cuda-8.0
module load python/2/7


cd /fslhome/tarch/compute/673/wordrnn

python train.py --data_dir ./data/original --rnn_size 256 --num_layers 2 --model lstm --batch_size 256 --seq_length 30 --num_epochs 5000  --save_dir "./save/" --init_from "./save/"
python sample.py  --save_dir ./save --pick 2 --prime "marriage is death" -n 20


python train.py --data_dir ./data/large --rnn_size 196 --num_layers 2 --model gru --batch_size 256 --seq_length 40 --num_epochs 5000  --save_dir "./save/LARGE"  --init_from "./save/LARGE"
python sample.py  --save_dir ./save/LARGE --pick 1 --prime "marriage is death" -n 40



# This is the endword trained, then retrained
python ./sample.py  --save_dir ./save/THEBONUSX_MASTER --pick 1 -n 40 --prime "ocean is black, marriage is death" -e "ocean"

# Regular new version
python ./sample.py  --save_dir ./save/THEBONUSX5 --pick 1 -n 40 --prime "ocean is black, marriage is death" -e ocean

# No bonus, original corpus
python ./sample.py  --save_dir ./save/NOBONUSX1 --pick 1 -n 40 --prime "ocean is black, marriage is death" -e ocean

# Original
python ../wordrnn/sample.py  --save_dir ../wordrnn/save/LARGE --pick 1 --prime "marriage is death" -n 40


## NEXT TRY:
 python train.py --data_dir ./data/original --rnn_size 256 --num_layers 2 --model lstm --batch_size 256 --seq_length 30 --num_epochs 5000 --sample true --bonus true  --save_dir "./save/THEBONUS2" --init_from "./save/THEBONUS2"
 
python ./sample.py -e turtle -o sample.txt -s ./save/THEBONUS2 --pick 1 -n 200
 
 
 
 
-bash-4.1$ sbatch ./train.sh
Submitted batch job 24061048
-bash-4.1$ sbatch ./train2.sh
Submitted batch job 24061049
-bash-4.1$ sbatch ./train_endword.sh
Submitted batch job 24061056



ORIGINAL:

-bash-4.1$ sbatch ./train.sh
Submitted batch job 24061052


scancel 24061050


# Test Tensorflow GPU
import tensorflow as tf
with tf.device('/gpu:0'):
    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
    c = tf.matmul(a, b)

with tf.Session() as sess:
    print (sess.run(c))